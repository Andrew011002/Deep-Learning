{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/Documents/Developing/Repos/Deep-Learning/deep-learning-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from utils import Dataset\n",
    "from training import train, predict, prompt\n",
    "from transformer import Transformer\n",
    "from tokenizer import WordPieceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"Jalen Hurts is the MVP\", \"doug\"]\n",
    "labels = [\"AJ Brown is best WR in the league\", \"doug as well\"]\n",
    "dataset = Dataset(inputs, labels)\n",
    "corpus = dataset.corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPieceTokenizer()\n",
    "tokenizer.train(50, corpus)\n",
    "tokenizer.pruncate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.tokenized(tokenizer, model=True)  \n",
    "dataloader = tokenized_dataset.dataloader(drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = tokenizer.size\n",
    "transformer = Transformer(n_tokens, 20, pad_idx=tokenizer[\"[PAD]\"])\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=1e-6, betas=(0.9, 0.98), eps=10e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(transformer, optimizer, dataloader, epochs=10, verbose=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict(transformer, [\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m], tokenizer[\u001b[39m\"\u001b[39;49m\u001b[39m[CLS]\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m20\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/Transformer/training.py:64\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, sequences, sos, maxlen, device)\u001b[0m\n\u001b[1;32m     61\u001b[0m softmax \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[39m# create src tensor(s)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m src \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(sequences)\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device) \u001b[39m# (unknown, src_len)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m src \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39mdim() \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m src \u001b[39m# (batch_size, src_len)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m src_mask \u001b[39m=\u001b[39m (src \u001b[39m!=\u001b[39m model\u001b[39m.\u001b[39mpad_idx)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got list)"
     ]
    }
   ],
   "source": [
    "predict(transformer, np.array([0, 1, 2, 3]), tokenizer[\"[CLS]\"], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##hhhhhhhhhhhhhhhhhhhh'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt(transformer, tokenizer, tokenizer[\"[CLS]\"], tokenizer[\"[SEP]\"], maxlen=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deep-learning-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2a551476f66aae3613481d49d8fd33a804395c218c6f6a2807839cdafe1d12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
