{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/Documents/Developing/Repos/Deep-Learning/deep-learning-venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from transformer import Transformer\n",
    "from metrics import Evaluator\n",
    "from datasets import load_dataset\n",
    "from utils import *\n",
    "from training import *\n",
    "from tokenizer import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset opus100 (/Users/tonimo/.cache/huggingface/datasets/opus100/de-en/0.0.0/256f3196b69901fb0c79810ef468e2c4ed84fbd563719920b1ff1fdc750f7704)\n",
      "Found cached dataset opus100 (/Users/tonimo/.cache/huggingface/datasets/opus100/de-en/0.0.0/256f3196b69901fb0c79810ef468e2c4ed84fbd563719920b1ff1fdc750f7704)\n"
     ]
    }
   ],
   "source": [
    "traindict = load_dataset(\"opus100\", \"de-en\", split=\"train\")\n",
    "testdict = load_dataset(\"opus100\", \"de-en\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_labels = get_split(traindict, \"en\", \"de\", size=100)\n",
    "test_inputs, test_labels = get_split(testdict, \"en\", \"de\", size=10)\n",
    "trainset = Dataset(train_inputs, train_labels)\n",
    "testset = Dataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's greed that it's gonna be the death of you...</td>\n",
       "      <td>Deine Habgier wird noch dein Tod sein.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vega.</td>\n",
       "      <td>- Vega.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just say when.</td>\n",
       "      <td>Sagen Sie einfach stopp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Wait.</td>\n",
       "      <td>- Warte.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't wanna be here.</td>\n",
       "      <td>Ich will nicht hier sein.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0  It's greed that it's gonna be the death of you...   \n",
       "1                                              Vega.   \n",
       "2                                     Just say when.   \n",
       "3                                            - Wait.   \n",
       "4                             I don't wanna be here.   \n",
       "\n",
       "                                   labels  \n",
       "0  Deine Habgier wird noch dein Tod sein.  \n",
       "1                                 - Vega.  \n",
       "2                Sagen Sie einfach stopp.  \n",
       "3                                - Warte.  \n",
       "4               Ich will nicht hier sein.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainframe = trainset.dataframe()\n",
    "trainframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By clicking on 'Save profile', you the user ag...</td>\n",
       "      <td>Die Nutzungsbedingungen werden durch das Klick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I wanted to show you something first.</td>\n",
       "      <td>Ich wollte dir erst noch etwas zeigen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You have suffered because of Shinkichi.</td>\n",
       "      <td>Du musstest wegen Shinkichi leiden.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moodle:bg-bab: Calendar: Day view: Friday, 25 ...</td>\n",
       "      <td>moodle:bg-bab: Kalender: Tagesansicht: Freitag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I mean, most people, they see another person w...</td>\n",
       "      <td>Ich meine, die meisten Leuten sehen eine ander...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0  By clicking on 'Save profile', you the user ag...   \n",
       "1              I wanted to show you something first.   \n",
       "2            You have suffered because of Shinkichi.   \n",
       "3  moodle:bg-bab: Calendar: Day view: Friday, 25 ...   \n",
       "4  I mean, most people, they see another person w...   \n",
       "\n",
       "                                              labels  \n",
       "0  Die Nutzungsbedingungen werden durch das Klick...  \n",
       "1             Ich wollte dir erst noch etwas zeigen.  \n",
       "2                Du musstest wegen Shinkichi leiden.  \n",
       "3  moodle:bg-bab: Kalender: Tagesansicht: Freitag...  \n",
       "4  Ich meine, die meisten Leuten sehen eine ander...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testframe = testset.dataframe()\n",
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>It's greed that it's gonna be the death of you...</td>\n",
       "      <td>Deine Habgier wird noch dein Tod sein.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   inputs  \\\n",
       "count                                                 100   \n",
       "unique                                                100   \n",
       "top     It's greed that it's gonna be the death of you...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                        labels  \n",
       "count                                      100  \n",
       "unique                                     100  \n",
       "top     Deine Habgier wird noch dein Tod sein.  \n",
       "freq                                         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainframe.isnull().values.any())\n",
    "trainframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>By clicking on 'Save profile', you the user ag...</td>\n",
       "      <td>Die Nutzungsbedingungen werden durch das Klick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   inputs  \\\n",
       "count                                                  10   \n",
       "unique                                                 10   \n",
       "top     By clicking on 'Save profile', you the user ag...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                   labels  \n",
       "count                                                  10  \n",
       "unique                                                 10  \n",
       "top     Die Nutzungsbedingungen werden durch das Klick...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(testframe.isnull().values.any())\n",
    "testframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What do you say Marylin?', 'Oder was meinst du?')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The more pronounced rate of decline was recorded from the port of Olbia with -21.9%, while in Golfo Aranci traffic fell by 15.4% and 9.8% of Porto Torres.',\n",
       "  '\" In den ersten fünf Monaten des Jahres 2011 haben sich die Sarden drei Häfen abgewickelt insgesamt 781.193 Passagiere, ein Rückgang von 175.954 Einheiten (-18,4%) gegenüber dem gleichen Zeitraum des Vorjahres. Je ausgeprägter der Rückgang wurde aus dem Hafen von Olbia mit -21,9% verzeichnet, während in Golfo Aranci Verkehr sank um 15,4% und 9,8% von Porto Torres.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = trainset.corpus() + testset.corpus()\n",
    "tokenizer = Nerdimizer()\n",
    "tokenizer.train(corpus, size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word piece tokens: 1572\n",
      "Maxlen: 46\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "maxlen = min(trainset.maxlen(tokenizer, factor=1), 150)\n",
    "start, end, pad = tokenizer[\"[S]\"], tokenizer[\"[E]\"], tokenizer[\"[P]\"]\n",
    "tokenizer.padon(maxlen, pad_id=pad, end=True)\n",
    "tokenizer.truncon(maxlen, end=True)\n",
    "print(f\"Number of word piece tokens: {vocab_size}\\nMaxlen: {maxlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedset = trainset.tokenized(tokenizer)\n",
    "dataloader = tokenizedset.dataloader(batch_size=32, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_size, maxlen, pad_id=pad, dm=256, nhead=8, layers=3, dff=1024)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.98), eps=10e-9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.9, patience=10)\n",
    "evaluator = Evaluator(testset, tokenizer, start, end, maxlen, sample=10, ngrams=4, threshold=20, \n",
    "                    mode=\"geometric\", verbose=True, device=device)\n",
    "checkpoint = Checkpoint(model, optimizer, scheduler, evaluator, epochs=50, \n",
    "                    path=\"checkpoints/english-german\", overwrite=False, verbose=True)\n",
    "clock = Clock()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n",
      "Epoch 1 Started\n",
      "Epoch 1 Complete | Epoch Loss: 7.4165\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:08 | Elapsed Time: 00:00:08\n",
      "Epoch 2 Started\n",
      "Epoch 2 Complete | Epoch Loss: 7.2613\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:03 | Elapsed Time: 00:00:11\n",
      "Epoch 3 Started\n",
      "Epoch 3 Complete | Epoch Loss: 7.0948\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:13\n",
      "Epoch 4 Started\n",
      "Epoch 4 Complete | Epoch Loss: 6.9579\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:15\n",
      "Epoch 5 Started\n",
      "Epoch 5 Complete | Epoch Loss: 6.8488\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:17\n",
      "Epoch 6 Started\n",
      "Epoch 6 Complete | Epoch Loss: 6.7514\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:19\n",
      "Epoch 7 Started\n",
      "Epoch 7 Complete | Epoch Loss: 6.6753\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:22\n",
      "Epoch 8 Started\n",
      "Epoch 8 Complete | Epoch Loss: 6.6221\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:24\n",
      "Epoch 9 Started\n",
      "Epoch 9 Complete | Epoch Loss: 6.5475\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:26\n",
      "Epoch 10 Started\n",
      "Epoch 10 Complete | Epoch Loss: 6.4863\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:28\n",
      "Epoch 11 Started\n",
      "Epoch 11 Complete | Epoch Loss: 6.4302\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:30\n",
      "Epoch 12 Started\n",
      "Epoch 12 Complete | Epoch Loss: 6.3933\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:32\n",
      "Epoch 13 Started\n",
      "Epoch 13 Complete | Epoch Loss: 6.3701\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:34\n",
      "Epoch 14 Started\n",
      "Epoch 14 Complete | Epoch Loss: 6.3319\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:36\n",
      "Epoch 15 Started\n",
      "Epoch 15 Complete | Epoch Loss: 6.3008\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:38\n",
      "Epoch 16 Started\n",
      "Epoch 16 Complete | Epoch Loss: 6.2714\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:40\n",
      "Epoch 17 Started\n",
      "Epoch 17 Complete | Epoch Loss: 6.2340\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:43\n",
      "Epoch 18 Started\n",
      "Epoch 18 Complete | Epoch Loss: 6.2074\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:45\n",
      "Epoch 19 Started\n",
      "Epoch 19 Complete | Epoch Loss: 6.1939\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:47\n",
      "Epoch 20 Started\n",
      "Epoch 20 Complete | Epoch Loss: 6.1849\n",
      "Evaluator Metric | BLEU: 0.00\n",
      "Epoch Duration: 00:02 | Elapsed Time: 00:00:49\n",
      "Epoch 21 Started\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(dataloader, model, optimizer, scheduler, evaluator, checkpoint, clock,\n\u001b[1;32m      2\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, warmups\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/Transformer/training.py:41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, optimizer, scheduler, evaluator, checkpoint, clock, epochs, warmups, verbose, device)\u001b[0m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     40\u001b[0m \u001b[39m# get prediction and reshape outputs\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m pred \u001b[39m=\u001b[39m model(src, tgt, src_mask\u001b[39m=\u001b[39;49msrc_mask, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask) \u001b[39m# shape: (batch_size, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m pred, out \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, pred\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), out\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# shape pred: (batch_size * seq_len, vocab_size) out: (batch_size * seq_len)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# calculate loss and backpropagate\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/deep-learning-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/Transformer/transformer.py:22\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, tgt, src_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tgt_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     19\u001b[0m     \u001b[39m# inshape: (batch_size, seq_len)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m     \u001b[39m# encode embeddings shape: e_out (batch_size, seq_len, dm)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     e_out, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, src_mask\u001b[39m=\u001b[39;49msrc_mask)\n\u001b[1;32m     24\u001b[0m     \u001b[39m# decode embeddings shape: d_out (batch_size, seq_len, dm)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     d_out, attn1, attn2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(e_out, tgt, src_mask\u001b[39m=\u001b[39msrc_mask, tgt_mask\u001b[39m=\u001b[39mtgt_mask)\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/deep-learning-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/Transformer/layers.py:48\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# pass src through stack of encoders (out of layer is in for next)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m encoder \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack:\n\u001b[0;32m---> 48\u001b[0m     x, attn \u001b[39m=\u001b[39m encoder(x, src_mask\u001b[39m=\u001b[39;49msrc_mask)\n\u001b[1;32m     49\u001b[0m out \u001b[39m=\u001b[39m x\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m out, attn\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/deep-learning-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/Transformer/layers.py:26\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     23\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m x_out)\n\u001b[1;32m     25\u001b[0m \u001b[39m# calc linear transforms then add & norm (residual connections) shape: (batch_size, seq_len, dm)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m x_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeedforward(x)\n\u001b[1;32m     27\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m x_out)\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m out, attn\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/deep-learning-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/Transformer/sublayers.py:102\u001b[0m, in \u001b[0;36mFeedForwardNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     99\u001b[0m     \u001b[39m# inshape (batch_size, seq_len, dm)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \n\u001b[1;32m    101\u001b[0m     \u001b[39m# first linear transform with ReLU shape: (batch_size, seq_len, dff)\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw1(x))\n\u001b[1;32m    104\u001b[0m     \u001b[39m# second linear transform shape: (batch_size, seq_len, dm)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw2(x)\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/deep-learning-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/deep-learning-venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(dataloader, model, optimizer, scheduler, evaluator, checkpoint, clock,\n",
    "    epochs=1000, warmups=100, verbose=True, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdd6e1c2b78b644f0d9d9d71785509219b94538d762b98250c0a1db53509cbf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
