{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/Documents/Developing/Repos/Deep-Learning/deep-learning-venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from utils import Dataset, load_model, save_model\n",
    "from training import train, predict, prompt\n",
    "from datasets import load_dataset\n",
    "from tokenizer import Nerdimizer, save_tokenizer, load_tokenizer\n",
    "from transformer import Transformer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en2de-176cd02372067e72\n",
      "Found cached dataset europa_eac_tm (/Users/tonimo/.cache/huggingface/datasets/europa_eac_tm/en2de-176cd02372067e72/0.0.0/955b2501a836c2ea49cfe3e719aec65dcbbc3356bbbe53cf46f08406eb77386a)\n"
     ]
    }
   ],
   "source": [
    "datadict = load_dataset(\"europa_eac_tm\", language_pair=(\"en\", \"de\"), split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [pair[\"en\"] for pair in datadict[\"translation\"]]\n",
    "labels = [pair[\"de\"] for pair in datadict[\"translation\"]]\n",
    "dataset = Dataset(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nr. teachers/trainers</td>\n",
       "      <td>Anzahl Lehrer(innen)/Trainer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APPLICANT</td>\n",
       "      <td>ANTRAGSTELLERIN/ANTRAGSTELLER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The grant application will be processed by com...</td>\n",
       "      <td>Der Förderantrag wird elektronisch verarbeitet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To be signed by the person legally authorised ...</td>\n",
       "      <td>Unterschrift der Person, die rechtsverbindlich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DATE OF BIRTH</td>\n",
       "      <td>GEBURTSDATUM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0                              Nr. teachers/trainers   \n",
       "1                                          APPLICANT   \n",
       "2  The grant application will be processed by com...   \n",
       "3  To be signed by the person legally authorised ...   \n",
       "4                                      DATE OF BIRTH   \n",
       "\n",
       "                                              labels  \n",
       "0                       Anzahl Lehrer(innen)/Trainer  \n",
       "1                      ANTRAGSTELLERIN/ANTRAGSTELLER  \n",
       "2  Der Förderantrag wird elektronisch verarbeitet...  \n",
       "3  Unterschrift der Person, die rechtsverbindlich...  \n",
       "4                                       GEBURTSDATUM  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.dataframe()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4473</td>\n",
       "      <td>4473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4452</td>\n",
       "      <td>4312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Event</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       inputs labels\n",
       "count    4473   4473\n",
       "unique   4452   4312\n",
       "top     Event      x\n",
       "freq        2     29"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.isnull().values.any())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Social inclusion in higher education'], ['x'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word piece tokens: 15695\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset.corpus()\n",
    "tokenizer = Nerdimizer()\n",
    "tokenizer.train(corpus)\n",
    "vocab_size = len(tokenizer)\n",
    "maxlen = dataset.avg_tokenized_len(tokenizer, factor=3)\n",
    "start, end, pad = tokenizer[\"[S]\"], tokenizer[\"[E]\"], tokenizer[\"[P]\"]\n",
    "tokenizer.padon(maxlen, pad_id=pad)\n",
    "tokenizer.truncon(maxlen)\n",
    "print(f\"Number of word piece tokens: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.tokenized(tokenizer)\n",
    "dataloader = tokenized_dataset.dataloader(batch_size=64, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embeddings): Embeddings(\n",
       "    (embedding): Embedding(15695, 512, padding_idx=3)\n",
       "  )\n",
       "  (pos_encoder): PositionalEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (stack): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (stack): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (maskmultihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (norm3): Norm()\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (maskmultihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (norm3): Norm()\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (maskmultihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (norm3): Norm()\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (maskmultihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (norm3): Norm()\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (maskmultihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (norm3): Norm()\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (maskmultihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (multihead): MultiHeadAttention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (scaled_dot_prod_attn): ScaledDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): FeedForwardNetwork(\n",
       "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): Norm()\n",
       "        (norm2): Norm()\n",
       "        (norm3): Norm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(vocab_size=len(tokenizer), maxlen=maxlen, pad_id=pad, \n",
    "                    dm=512, nhead=8, layers=6, dff=2048)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.98), eps=10e-9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.9, patience=10)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model, optimizer, scheduler, dataloader, epochs=1000, warmups=100, verbose=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model, \"Transformer-Base\", \"saves/\")\n",
    "# save_tokenizer(tokenizer, \"Tokenizer-en-de\", \"saves/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from Tokenizer-en-de.json\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[39m=\u001b[39m load_tokenizer(\u001b[39m\"\u001b[39m\u001b[39mTokenizer-en-de\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msaves/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mload_state_dict(\u001b[39m\"\u001b[39;49m\u001b[39mTransformer-Base\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaves/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Developing/Repos/Deep-Learning/deep-learning-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1620\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1597\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Copies parameters and buffers from :attr:`state_dict` into\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \u001b[39mthis module and its descendants. If :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m \u001b[39mthe keys of :attr:`state_dict` must exactly match the keys returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[39m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   1618\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 1620\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected state_dict to be dict-like, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(state_dict)))\n\u001b[1;32m   1622\u001b[0m missing_keys: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[1;32m   1623\u001b[0m unexpected_keys: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'str'>."
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(\"Tokenizer-en-de\", \"saves/\")\n",
    "model = load_model(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdd6e1c2b78b644f0d9d9d71785509219b94538d762b98250c0a1db53509cbf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
