{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from transformer import Transformer\n",
    "from metrics import Evaluator\n",
    "from datasets import load_dataset\n",
    "from utils import *\n",
    "from train import *\n",
    "from inference import *\n",
    "from tokenizer import *\n",
    "from config import *\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindict = load_dataset(\"opus100\", \"de-en\", split=\"train\")\n",
    "testdict = load_dataset(\"opus100\", \"de-en\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_labels = get_split(traindict, \"en\", \"de\", size=train_size)\n",
    "test_inputs, test_labels = get_split(testdict, \"en\", \"de\", size=test_size)\n",
    "trainset = Dataset(train_inputs, train_labels)\n",
    "testset = Dataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainframe = trainset.dataframe()\n",
    "trainframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testframe = testset.dataframe()\n",
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainframe.isnull().values.any())\n",
    "trainframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testframe.isnull().values.any())\n",
    "testframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_en = trainset.corpus(data=\"inputs\") + testset.corpus(data=\"inputs\")\n",
    "corpus_de = trainset.corpus(data=\"labels\") + testset.corpus(data=\"labels\")\n",
    "tokenizer_en = Nerdimizer()\n",
    "tokenizer_de = Nerdimizer()\n",
    "tokenizer_en.train(corpus_en, size=vocab_size_english)\n",
    "tokenizer_de.train(corpus_de, size=vocab_size_german)\n",
    "translator = Translator(tokenizer_en, tokenizer_de)\n",
    "save_tokenizer(translator, \"translator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab, de_vocab = translator.vocab_size()\n",
    "start, end, pad = translator[\"[S]\"], translator[\"[E]\"], translator[\"[P]\"]\n",
    "print(f\"Number of input tokens: {en_vocab}\\nNumber of output tokens: {de_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.padon(maxlen, end=True, pad_id=pad)\n",
    "translator.truncon(maxlen, end=True)\n",
    "tokenized_trainset = trainset.tokenized(translator, model=True)\n",
    "dataloader = tokenized_trainset.dataloader(batch_size=batch_size, drop_last=False)\n",
    "print(f\"Maxlen: {maxlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(en_vocab, de_vocab, maxlen, pad_id=pad, dm=dm, nhead=nhead, layers=layers, dff=dff,\n",
    "                    bias=bias, dropout=dropout, eps=eps)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas, eps=adam_eps)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=factor, patience=patience)\n",
    "search = Beam(model, start, end, maxlen, beam_width=beam_width, breadth=max_breadth, \n",
    "                mode=search_mode, alpha=alpha, device=device)\n",
    "evaluator = Evaluator(testset, translator, search, sample=sample_size, \n",
    "                        ngrams=ngrams, bleu_goal=bleu_goal, mode=\"geometric\")\n",
    "clock = Clock()\n",
    "checkpoint = Checkpoint(dataloader, model, optimizer, scheduler, evaluator, clock, \n",
    "                        epochs=save_every, path=\"checkpoint\", overwrite=overwrite)\n",
    "model.to(device)\n",
    "print(f\"Number of Trainable Paramaters: {parameter_count(model):.1f}M\\nSize of Model: {model_size(model):.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataloader, model, optimizer, scheduler, evaluator, checkpoint, clock,\n",
    "    epochs=epochs, warmups=warmups, verbose=verbose, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdd6e1c2b78b644f0d9d9d71785509219b94538d762b98250c0a1db53509cbf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
