{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from transformer import Transformer\n",
    "from metrics import Evaluator\n",
    "from datasets import load_dataset\n",
    "from utils import *\n",
    "from training import *\n",
    "from tokenizer import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindict = load_dataset(\"opus100\", \"de-en\", split=\"train\")\n",
    "testdict = load_dataset(\"opus100\", \"de-en\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_labels = get_split(traindict, \"en\", \"de\", size=100000)\n",
    "test_inputs, test_labels = get_split(testdict, \"en\", \"de\", size=1000)\n",
    "trainset = Dataset(train_inputs, train_labels)\n",
    "testset = Dataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainframe = trainset.dataframe()\n",
    "trainframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testframe = testset.dataframe()\n",
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainframe.isnull().values.any())\n",
    "trainframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testframe.isnull().values.any())\n",
    "testframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_en = trainset.corpus(data=\"inputs\") + testset.corpus(data=\"inputs\")\n",
    "corpus_de = trainset.corpus(data=\"labels\") + testset.corpus(data=\"labels\")\n",
    "tokenizer_en = Nerdimizer()\n",
    "tokenizer_de = Nerdimizer()\n",
    "tokenizer_en.train(corpus_en, size=25000)\n",
    "tokenizer_de.train(corpus_de, size=25000)\n",
    "translator = Translator(tokenizer_en, tokenizer_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab, de_vocab = translator.vocab_size()\n",
    "maxlen_train = trainset.maxlen(translator)\n",
    "maxlen_test = testset.maxlen(translator)\n",
    "maxlen = min(maxlen_train, maxlen_test, 256)\n",
    "start, end, pad = tokenizer_en[\"[S]\"], tokenizer_en[\"[E]\"], tokenizer_en[\"[P]\"]\n",
    "print(f\"Number of input tokens: {len(tokenizer_en)}\\nNumber of output tokens: {len(tokenizer_de)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.padon(maxlen, end=True, pad_id=pad)\n",
    "translator.truncon(maxlen, end=True)\n",
    "print(f\"Maxlen: {maxlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(en_vocab, de_vocab, maxlen, pad_id=pad, dm=512, nhead=8, layers=6, dff=2048)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.98), eps=10e-9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.9, patience=10)\n",
    "evaluator = Evaluator(testset, translator, \"[S]\", \"[E]\", maxlen, sample=50, ngrams=4, threshold=25, \n",
    "                    mode=\"geometric\", device=device)\n",
    "clock = Clock()\n",
    "checkpoint = Checkpoint(model, optimizer, scheduler, evaluator, clock, epochs=100, \n",
    "                    path=\"english-german\", overwrite=True)\n",
    "model.to(device)\n",
    "print(f\"Number of Trainable Paramaters: {parameter_count(model):.1f}M\\nSize of Model: {model_size(model):.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_trainset = trainset.tokenized(translator, model=True)\n",
    "dataloader = tokenized_trainset.dataloader(batch_size=128, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(dataloader, model, optimizer, scheduler, evaluator, checkpoint, clock,\n",
    "    # epochs=1000, warmups=100, verbose=True, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1 (v3.10.1:2cd268a3a9, Dec  6 2021, 14:28:59) [Clang 13.0.0 (clang-1300.0.29.3)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdd6e1c2b78b644f0d9d9d71785509219b94538d762b98250c0a1db53509cbf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
